{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "final_project_499",
   "display_name": "Python 3.8  ('venv': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import pytesseract\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 443/443 [00:00<00:00, 569kB/s]\n",
      "Downloading: 100%|██████████| 1.34G/1.34G [03:50<00:00, 5.81MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 6.92MB/s]\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 31.0kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 4.37MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ACUTE TOXICITY IN MICE\n\n \n\n3-Hydroxy-3-methylbutanoic acid (Tur _13)\n\n \n\n \n\n \n\ncourcuno\n\ncouee —LOXillara - Organic Chemistry 9. sop yo __OR39-23 vorno AY.\n573/79\n\nate necenen —_UNK« reste 12/28/78 seronreo 10/6/80, Update\n\nwesncxtons H. S. Tong & M. S. Forte! notesooxeace —BIOL4-23\n\n \n\nenn — gn bd Gti (hie 0 Foote)\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nsraan or wee SWiss-Webster wus * see bare necengo, UX +\nAVERAGE WEGHTRANGE (OM source Camm Research\nour oF courowno Aounistaron aro ow ow CD mwataron\ncowPowno VERE 3), 5 METI. cEULOSE 3 com on ose power\ncro wo. x sown some Ses,\ncota Bcc 0 CEN ese0)\n_ 1800 1/6\n: 10 2160 0/6\n* 10 2592 0/6\n‘ 10 3732 3/6\n. 10 4479 6/6\n\n \n\n \n\n \n\nrcrenence roncaousnon Litchfield, J. T. and Wilcoxin, F., J. of Pharmacol.\n\nand Exper. Ther., 90:99, 1948.\n\n \n\ntos ose conroence umatsy_3-5 (3.1 to 3.9) g/kg\nconcwson THAs compound appears to act _as a CNS depressant with symptons\nof respiratory depression, constriction of blood vessels, and in-\nactivity. Survivors recovered in 48 hours. The recommended safe\n\ndose for a single trial by inhalation in man is 0.3 mg.\n\n \n\n3\n| Copies to the Following: Dr. H. J. Minnemeyer 3\nHs. L.-B. Gray <\n\nTORUARD RESEARON CENTER Pom (5)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"./noisy_documents_dataset/training_data/images/00040534.png\")\n",
    "results = pytesseract.image_to_string(img)\n",
    "question = \"source?\"\n",
    "answer_text = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[CLS]           101\nsource        3,120\n\n[SEP]           102\n\nacute        11,325\ntoxicity     22,423\nin            1,999\nmice         12,328\n3             1,017\n-             1,011\nhydro        18,479\n##xy         18,037\n-             1,011\n3             1,017\n-             1,011\nmethyl       25,003\n##bu          8,569\n##tan         5,794\n##oic        19,419\nacid          5,648\n(             1,006\ntu           10,722\n##r           2,099\n_             1,035\n13            2,410\n)             1,007\nco            2,522\n##ur          3,126\n##cu         10,841\n##no          3,630\nco            2,522\n##ue          5,657\n##e           2,063\n—             1,517\nlo            8,840\n##xi          9,048\n##llar       17,305\n##a           2,050\n-             1,011\norganic       7,554\nchemistry     6,370\n9             1,023\n.             1,012\nso            2,061\n##p           2,361\nyo           10,930\n_             1,035\n_             1,035\nor            2,030\n##39         23,499\n-             1,011\n23            2,603\nvo           29,536\n##rno        19,139\na             1,037\n##y           2,100\n.             1,012\n57            5,401\n##3           2,509\n/             1,013\n79            6,535\nate           8,823\nnec          26,785\n##ene         8,625\n##n           2,078\n—             1,517\n_             1,035\nun            4,895\n##k           2,243\n«             1,077\nrest          2,717\n##e           2,063\n12            2,260\n/             1,013\n28            2,654\n/             1,013\n78            6,275\nser          14,262\n##on          2,239\n##re          2,890\n##o           2,080\n10            2,184\n/             1,013\n6             1,020\n/             1,013\n80            3,770\n,             1,010\nupdate       10,651\nwes          14,008\n##nc         12,273\n##xton       14,226\n##s           2,015\nh             1,044\n.             1,012\ns             1,055\n.             1,012\ntong         15,740\n&             1,004\nm             1,049\n.             1,012\ns             1,055\n.             1,012\nforte        24,898\n!               999\nnotes         3,964\n##oo          9,541\n##x           2,595\n##ea          5,243\n##ce          3,401\n—             1,517\nbio          16,012\n##l           2,140\n##4           2,549\n-             1,011\n23            2,603\nen            4,372\n##n           2,078\n—             1,517\ng             1,043\n##n           2,078\nb             1,038\n##d           2,094\ngt           14,181\n##i           2,072\n(             1,006\nhi            7,632\n##e           2,063\n0             1,014\nfoot          3,329\n##e           2,063\n)             1,007\nsr            5,034\n##aan        14,634\nor            2,030\nwee          16,776\nswiss         5,364\n-             1,011\nwebster      11,635\nwu            8,814\n##s           2,015\n*             1,008\nsee           2,156\nbare          6,436\nnec          26,785\n##eng        13,159\n##o           2,080\n,             1,010\nu             1,057\n##x           2,595\n+             1,009\naverage       2,779\nwe            2,057\n##ght        13,900\n##rang       24,388\n##e           2,063\n(             1,006\nom           18,168\nsource        3,120\ncam          11,503\n##m           2,213\nresearch      2,470\nour           2,256\nof            1,997\nco            2,522\n##uro        10,976\n##wn          7,962\n##o           2,080\nao           20,118\n##uni        19,496\n##star       14,117\n##on          2,239\nar           12,098\n##o           2,080\now           27,593\now           27,593\ncd            3,729\nmw           12,464\n##ata         6,790\n##ron         4,948\ncow          11,190\n##po          6,873\n##wn          7,962\n##o           2,080\nve            2,310\n##re          2,890\n3             1,017\n)             1,007\n,             1,010\n5             1,019\nmet           2,777\n##i           2,072\n.             1,012\nce            8,292\n##ulo        18,845\n##se          3,366\n3             1,017\ncom           4,012\non            2,006\nos            9,808\n##e           2,063\npower         2,373\ncr           13,675\n##o           2,080\nwo           24,185\n.             1,012\nx             1,060\nso            2,061\n##wn          7,962\nsome          2,070\nse            7,367\n##s           2,015\n,             1,010\ncot          26,046\n##a           2,050\nbc            4,647\n##c           2,278\n0             1,014\nce            8,292\n##n           2,078\nes            9,686\n##e           2,063\n##0           2,692\n)             1,007\n_             1,035\n1800          9,807\n1             1,015\n/             1,013\n6             1,020\n:             1,024\n10            2,184\n216          20,294\n##0           2,692\n0             1,014\n/             1,013\n6             1,020\n*             1,008\n10            2,184\n259          25,191\n##2           2,475\n0             1,014\n/             1,013\n6             1,020\n‘             1,520\n10            2,184\n37            4,261\n##32         16,703\n3             1,017\n/             1,013\n6             1,020\n.             1,012\n10            2,184\n44            4,008\n##7           2,581\n##9           2,683\n6             1,020\n/             1,013\n6             1,020\nrc           22,110\n##ren         7,389\n##ence       10,127\nron           6,902\n##cao        20,808\n##us          2,271\n##non         8,540\nlit           5,507\n##chfield    22,693\n,             1,010\nj             1,046\n.             1,012\nt             1,056\n.             1,012\nand           1,998\nwilcox       23,926\n##in          2,378\n,             1,010\nf             1,042\n.             1,012\n,             1,010\nj             1,046\n.             1,012\nof            1,997\nph            6,887\n##arm        27,292\n##aco        22,684\n##l           2,140\n.             1,012\nand           1,998\nex            4,654\n##per         4,842\n.             1,012\nthe           1,996\n##r           2,099\n.             1,012\n,             1,010\n90            3,938\n:             1,024\n99            5,585\n,             1,010\n1948          3,882\n.             1,012\nto            2,000\n##s           2,015\nos            9,808\n##e           2,063\ncon           9,530\n##ro          3,217\n##ence       10,127\num            8,529\n##ats        11,149\n##y           2,100\n_             1,035\n3             1,017\n-             1,011\n5             1,019\n(             1,006\n3             1,017\n.             1,012\n1             1,015\nto            2,000\n3             1,017\n.             1,012\n9             1,023\n)             1,007\ng             1,043\n/             1,013\nkg            4,705\ncon           9,530\n##c           2,278\n##ws          9,333\n##on          2,239\ntha          22,794\n##s           2,015\ncompound      7,328\nappears       3,544\nto            2,000\nact           2,552\n_             1,035\nas            2,004\na             1,037\ncn           27,166\n##s           2,015\nde            2,139\n##press      20,110\n##ant         4,630\nwith          2,007\nsy           25,353\n##mpton      26,793\n##s           2,015\nof            1,997\nrespiratory  16,464\ndepression    6,245\n,             1,010\ncon           9,530\n##st          3,367\n##ric         7,277\n##tion        3,508\nof            1,997\nblood         2,668\nvessels       6,470\n,             1,010\nand           1,998\nin            1,999\n-             1,011\nactivity      4,023\n.             1,012\nsurvivors     8,643\nrecovered     6,757\nin            1,999\n48            4,466\nhours         2,847\n.             1,012\nthe           1,996\nrecommended   6,749\nsafe          3,647\ndose         13,004\nfor           2,005\na             1,037\nsingle        2,309\ntrial         3,979\nby            2,011\nin            1,999\n##hala       19,531\n##tion        3,508\nin            1,999\nman           2,158\nis            2,003\n0             1,014\n.             1,012\n3             1,017\nmg           11,460\n.             1,012\n3             1,017\n|             1,064\ncopies        4,809\nto            2,000\nthe           1,996\nfollowing     2,206\n:             1,024\ndr            2,852\n.             1,012\nh             1,044\n.             1,012\nj             1,046\n.             1,012\nmin           8,117\n##nem        25,832\n##eyer       20,211\n3             1,017\nhs           26,236\n.             1,012\nl             1,048\n.             1,012\n-             1,011\nb             1,038\n.             1,012\ngray          3,897\n<             1,026\ntor          17,153\n##ua          6,692\n##rd          4,103\nres          24,501\n##ear        14,644\n##on          2,239\ncenter        2,415\npo           13,433\n##m           2,213\n(             1,006\n5             1,019\n)             1,007\n\n[SEP]           102\n\n"
     ]
    }
   ],
   "source": [
    "input_id = tokenizer.encode(question , answer_text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id)\n",
    "for token, id in zip(tokens, input_id):\n",
    "    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    \n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_index = input_id.index(tokenizer.sep_token_id)\n",
    "num_seg_a = sep_index + 1\n",
    "num_seg_b = len(input_id) - num_seg_a\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "assert len(segment_ids) == len(input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 2.3660, -7.2117,  2.3660, -6.0958, -6.2897, -8.1140, -7.1386, -5.9688,\n         -8.0855, -7.1902, -8.0850, -8.0483, -6.5638, -8.3805, -7.4542, -8.2954,\n         -8.5292, -8.6881, -7.3928, -7.9626, -7.2642, -8.6811, -8.8174, -8.0639,\n         -8.5304, -7.4773, -8.7439, -8.7020, -9.0545, -8.1495, -8.9114, -9.1094,\n         -8.4536, -7.3954, -8.5220, -8.7728, -8.8887, -8.5008, -7.4519, -7.5211,\n         -7.4831, -9.1270, -7.3575, -8.8682, -7.6301, -8.5815, -8.3570, -8.0465,\n         -8.8421, -8.7255, -8.8455, -8.0035, -9.0153, -8.0619, -8.8927, -9.0339,\n         -8.3502, -8.9042, -8.7094, -8.7032, -7.8170, -8.3467, -8.9157, -9.0819,\n         -8.2602, -7.7605, -7.5585, -8.7778, -7.6383, -8.0928, -9.1710, -8.2573,\n         -8.7339, -8.6789, -8.7905, -8.7831, -8.1261, -8.7849, -8.9128, -9.2386,\n         -8.1815, -8.7317, -8.7581, -8.7778, -8.8550, -8.7122, -7.5243, -7.8570,\n         -8.5700, -8.7967, -9.1794, -7.9989, -8.6893, -8.3963, -8.8266, -8.7326,\n         -8.8093, -8.1457, -8.5828, -8.5820, -8.9266, -8.7552, -8.5719, -7.3389,\n         -8.4274, -8.5264, -8.6382, -8.8883, -8.6000, -7.1759, -8.7160, -8.8501,\n         -8.7408, -8.7198, -7.9246, -9.0219, -8.7217, -7.4660, -8.9334, -8.2269,\n         -9.0514, -7.7588, -9.1287, -8.3734, -7.9608, -9.1539, -8.8073, -8.5049,\n         -9.3316, -9.2070, -8.1567, -9.2731, -8.6538, -8.2424, -8.3374, -8.8166,\n         -8.7653, -8.2845, -9.2410, -8.1340, -8.0474, -7.7909, -8.2819, -8.7190,\n         -9.1936, -8.6020, -7.5869, -8.9057, -8.6508, -7.8340, -8.1551, -8.8174,\n         -8.7546, -9.2768, -7.8720, -6.8306, -7.5489, -4.3744, -7.9643, -6.5629,\n         -5.6421, -7.8024, -7.0192, -8.6685, -8.7778, -9.1242, -7.9094, -8.6634,\n         -8.7176, -9.0872, -7.6323, -9.0134, -8.0529, -8.0582, -7.8703, -7.6850,\n         -8.4124, -8.8742, -7.8057, -8.5177, -8.7911, -9.1525, -8.1637, -9.0835,\n         -8.5473, -8.0538, -8.4761, -7.7941, -7.9142, -8.7378, -9.1268, -7.8008,\n         -8.7408, -9.0795, -8.6155, -7.8234, -8.1574, -7.6336, -8.8923, -7.8878,\n         -8.2023, -9.1000, -8.1658, -8.9783, -8.2134, -8.0694, -8.8336, -8.0566,\n         -8.3674, -8.8901, -8.4981, -7.4721, -8.8785, -8.0694, -8.9100, -8.7092,\n         -8.2441, -8.9909, -8.2346, -8.7420, -9.1810, -8.2694, -8.4752, -8.4400,\n         -8.1392, -8.7774, -8.8213, -8.9239, -8.6759, -8.3946, -8.9504, -8.4129,\n         -8.7374, -8.7932, -8.5825, -8.4667, -8.4716, -8.9650, -8.3784, -8.7011,\n         -8.7523, -8.2285, -8.4939, -8.3868, -8.9994, -8.4136, -8.6842, -8.6548,\n         -8.8941, -8.5619, -8.4715, -8.8280, -8.9852, -8.5273, -8.8115, -8.6704,\n         -7.6183, -8.6045, -8.8060, -7.7786, -8.6437, -8.7783, -8.9561, -8.1686,\n         -8.8940, -8.8357, -8.1481, -8.6401, -8.7330, -9.0455, -8.4606, -8.0717,\n         -9.1155, -8.9720, -8.5789, -9.0961, -8.8572, -7.7088, -8.9665, -8.7477,\n         -8.3180, -8.7096, -8.8016, -9.0447, -9.1468, -8.7712, -8.0045, -8.9022,\n         -9.1292, -8.4911, -9.0138, -9.1283, -8.9045, -8.2908, -8.9355, -8.8715,\n         -8.8010, -7.9512, -8.4049, -7.6177, -8.7372, -7.8227, -8.8241, -7.7890,\n         -8.6540, -8.5889, -7.6135, -8.6157, -8.8365, -8.7480, -8.4693, -8.5452,\n         -8.4383, -8.1848, -8.0167, -8.6716, -8.5946, -8.5886, -8.1291, -8.6797,\n         -8.7568, -9.0526, -8.2832, -8.6670, -8.3806, -7.7393, -8.5920, -8.6788,\n         -9.0126, -8.1662, -9.1120, -8.3905, -8.2383, -8.5758, -8.3516, -7.9243,\n         -8.5305, -8.3439, -7.8070, -8.9574, -8.1523, -8.4470, -8.6800, -8.5339,\n         -8.0882, -8.2378, -8.8759, -8.6815, -7.9691, -8.6670, -8.8275, -8.0821,\n         -8.5175, -8.4588, -8.7537, -8.6495, -8.3009, -8.8769, -8.9031, -8.7190,\n         -8.4020, -8.6372, -8.4570, -7.0643, -7.5025, -8.3319, -8.6192, -8.3357,\n         -8.6148, -8.1836, -7.2364, -7.0524, -8.0012, -7.8798, -8.6966, -8.3306,\n         -8.3913, -8.4938, -8.7574, -8.0708, -8.4514, -9.0166, -8.7148, -8.4080,\n         -8.4075, -6.9740, -8.2108, -8.0971, -7.2441, -8.2939, -7.1961, -7.4619,\n         -6.2959, -7.8030, -7.9376, -7.9953, -8.6093, -7.5322, -8.7632, -8.1538,\n         -8.6544, -8.4748, -8.8292, -8.3851, -8.7378, -8.9487, -8.3511, -7.9155,\n         -9.0131, -8.4460, -8.9879, -8.6447, -8.5022, -9.0197, -8.7518, -8.3230,\n         -7.9536, -8.5427, -8.9743, -8.3969, -8.8177, -9.0918, -8.5891, -7.9435,\n         -8.9471, -8.4770, -8.7322, -9.1023,  2.3660]],\n       grad_fn=<SqueezeBackward1>) tensor([[ 2.8741, -6.7918,  2.8741, -7.7053, -5.4536, -8.2734, -6.2429, -7.4073,\n         -7.8529, -7.9585, -7.4773, -7.9163, -6.9897, -7.5972, -7.1479, -7.4576,\n         -7.2558, -7.2858, -4.4422, -7.4857, -8.0399, -7.4859, -8.1861, -6.6209,\n         -6.1873, -7.8067, -7.7107, -7.9792, -7.3018, -7.9648, -7.5974, -6.9949,\n         -7.8348, -8.0667, -7.9311, -7.9097, -7.1842, -8.1788, -7.0057, -5.5032,\n         -6.0798, -7.7093, -8.2064, -7.3782, -7.2747, -7.1337, -7.8166, -8.0911,\n         -7.4435, -8.1978, -7.0517, -8.1252, -7.3560, -7.9514, -7.2731, -8.0272,\n         -8.0651, -7.4756, -8.2116, -7.2298, -7.2688, -8.1556, -7.9476, -7.6695,\n         -7.5645, -7.3029, -8.0810, -7.3002, -7.0028, -7.6023, -7.3267, -8.0966,\n         -8.1800, -8.0679, -8.2390, -7.5779, -7.9872, -7.9880, -7.9226, -7.6170,\n         -7.9523, -8.2564, -8.0793, -8.3308, -7.1093, -6.6558, -6.7190, -8.3587,\n         -8.2482, -8.2030, -7.6730, -8.0234, -8.0445, -7.8742, -8.1084, -7.4800,\n         -8.2327, -7.9273, -7.7598, -7.8253, -8.0439, -7.2053, -7.0128, -7.4669,\n         -8.1028, -8.3516, -8.3320, -7.8210, -8.4453, -7.9519, -7.9542, -7.9042,\n         -8.4174, -7.2615, -8.1010, -7.6180, -8.2753, -8.3701, -7.7991, -8.1597,\n         -7.7220, -8.0221, -7.2600, -8.1673, -8.2536, -7.7326, -7.7506, -8.0944,\n         -7.5468, -7.4007, -8.0827, -7.4535, -8.2888, -7.9671, -7.7628, -8.3245,\n         -6.8443, -7.9488, -7.2313, -7.3294, -7.8125, -7.1907, -8.2361, -7.8934,\n         -7.4707, -6.7413, -8.2778, -7.6503, -8.1730, -6.8387, -8.0222, -8.0413,\n         -8.1522, -7.2191, -7.3809, -6.4011, -7.5472, -7.4122, -6.6500, -6.5330,\n         -4.2280, -7.3465, -7.8782, -7.5697, -7.6716, -6.7880, -7.6804, -7.7160,\n         -7.5945, -6.2260, -7.4068, -6.6577, -6.8127, -7.2221, -7.1114, -7.9390,\n         -7.6378, -6.8966, -7.9071, -7.8306, -7.7948, -7.3599, -7.8496, -7.3271,\n         -6.5681, -6.4934, -6.5254, -7.0612, -7.5991, -7.1974, -7.3023, -7.8497,\n         -7.6659, -7.1092, -6.8661, -6.3389, -7.6207, -8.0642, -7.2564, -6.9544,\n         -7.7514, -7.3541, -7.0744, -7.7574, -7.4480, -8.1024, -7.2827, -7.1833,\n         -7.8065, -7.0352, -6.5158, -8.1056, -7.4837, -7.9292, -7.7171, -7.3759,\n         -8.0568, -7.6924, -7.9475, -7.8200, -7.2316, -7.0980, -7.7820, -6.6515,\n         -7.4948, -8.0480, -7.7099, -7.9791, -7.7516, -7.7784, -7.0362, -7.3231,\n         -8.0836, -7.4769, -8.0290, -7.7676, -7.6688, -7.2060, -7.2963, -8.0334,\n         -7.1287, -7.8357, -7.5896, -7.7930, -7.2782, -7.3577, -8.0426, -7.4345,\n         -7.9274, -7.4017, -7.7154, -7.7816, -7.3621, -7.1412, -7.8757, -5.9242,\n         -7.3270, -7.6647, -6.9748, -8.0661, -7.8090, -7.8648, -7.4800, -7.8321,\n         -7.1571, -7.3352, -7.6565, -7.9834, -7.7008, -7.2485, -7.9405, -7.6963,\n         -7.1439, -7.3007, -7.5866, -7.3064, -7.0856, -7.6021, -7.6412, -8.1118,\n         -7.9980, -7.6926, -7.9617, -7.9061, -7.7368, -8.1935, -8.0081, -7.7298,\n         -7.9079, -8.1184, -7.5849, -7.3343, -7.1128, -7.6896, -7.9958, -7.4081,\n         -7.2183, -7.2187, -6.8035, -7.7663, -7.3145, -7.9643, -7.3120, -8.1215,\n         -7.6521, -7.1125, -8.1115, -7.7460, -7.5477, -7.9224, -7.5485, -7.9917,\n         -6.7474, -7.8744, -7.6854, -7.8697, -7.3635, -7.9546, -7.6170, -7.8925,\n         -7.3920, -6.9670, -7.4613, -7.8499, -6.3115, -7.7632, -7.8838, -7.8327,\n         -7.4608, -7.6487, -6.9412, -6.9218, -7.9099, -7.6183, -7.5256, -6.7667,\n         -7.6891, -7.7447, -8.0573, -7.3284, -7.7898, -7.3228, -5.5335, -7.2943,\n         -8.0425, -7.1835, -7.1117, -7.7307, -7.4788, -6.4290, -6.8477, -7.9140,\n         -7.6803, -7.6737, -7.3471, -7.7712, -7.6867, -6.7996, -6.5899, -7.5091,\n         -7.5509, -7.7532, -6.2754, -5.2604, -6.5112, -7.2393, -7.6197, -7.8482,\n         -6.0629, -5.9759, -7.7846, -7.5780, -7.4647, -6.8905, -7.8035, -7.7907,\n         -7.8071, -6.8168, -7.8196, -7.8218, -7.2822, -6.8049, -7.8050, -6.8623,\n         -7.7914, -7.5372, -7.9856, -7.3408, -6.0155, -7.1248, -5.5264, -6.1766,\n         -5.6557, -7.0820, -7.7743, -7.4794, -7.3887, -7.7872, -7.8153, -7.9259,\n         -7.7399, -7.8274, -7.8122, -8.0552, -7.8205, -7.4302, -6.7176, -7.2549,\n         -7.6062, -7.6580, -7.4406, -8.0576, -7.6698, -7.6901, -6.8562, -7.5448,\n         -8.1473, -7.8555, -7.3806, -7.8903, -7.7893, -7.6419, -6.7495, -7.6067,\n         -7.1690, -8.0330, -7.1779, -6.9430,  2.8740]],\n       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "start_scores, end_scores = model(torch.tensor([input_id]), # The tokens representing our input text.\n",
    "                                 token_type_ids=torch.tensor([segment_ids])).values() # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "print(start_scores , end_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Answer: \"[CLS]\"\n"
     ]
    }
   ],
   "source": [
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "\n",
    "# Combine the tokens in the answer and print it out.\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}